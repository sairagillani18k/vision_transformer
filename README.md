# ğŸ¤– Real-Time Hand Gesture Recognition using Vision Transformers

This project demonstrates **real-time hand gesture recognition** by combining **MediaPipe** for hand tracking and a **Vision Transformer (ViT)** for gesture classification. It captures hand landmarks from a live webcam feed, processes them into input tensors, and feeds them into a transformer-based model to predict the user's hand gesture â€” all in real time.

---

## ğŸ“Œ Features

- ğŸ–ï¸ **Real-time hand tracking** using MediaPipe (Google)
- ğŸ§  **Gesture classification** using pretrained Vision Transformers (HuggingFace ViT)
- âš¡ Fast and lightweight pipeline suitable for modern hardware
- ğŸ§© Modular code structure â€” easy to train, test, and extend
- ğŸ¥ Live webcam-based inference with OpenCV

---

## ğŸ“ Project Structure

```
Real-Time-Hand-Gesture-Recognition/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ vit_model.py         # ViT model definition
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataset.py           # Dataset class for training
â”‚   â””â”€â”€ mediapipe_utils.py   # Landmark extraction using MediaPipe
â”œâ”€â”€ train.py                 # Training pipeline
â”œâ”€â”€ inference.py             # Real-time webcam inference
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore               # Ignore temp files and configs
â””â”€â”€ README.md                # You're here!
```

---

## ğŸš€ Setup

1. **Clone the repository**

```bash
git clone https://github.com/your-username/real-time-gesture-transformer.git
cd real-time-gesture-transformer
```

2. **Install dependencies**

```bash
pip install -r requirements.txt
```

> Requirements:
> - `torch`, `transformers` (for ViT model)
> - `opencv-python`, `mediapipe` (for hand tracking and webcam feed)
> - `numpy`

---

## ğŸ‹ï¸â€â™€ï¸ Training the Model

By default, the project includes a minimal training setup using dummy hand landmark data.

To train:

```bash
python train.py
```

- Loads dummy landmark data (replace this with your real dataset)
- Trains a ViT-based classifier
- Saves the trained model to `gesture_model.pth`

If you have real landmark vectors and gesture labels, update `train.py` with your data loading logic.

---

## ğŸ¯ Real-Time Inference

To run real-time gesture recognition with webcam:

```bash
python inference.py
```

- Starts webcam stream
- Detects hand landmarks using MediaPipe
- Classifies gestures using the trained Vision Transformer model
- Displays gesture predictions over the live video

Press `q` to quit the video window.

---

## ğŸ§  Model Architecture

We use a pretrained [ViT-B/16](https://huggingface.co/google/vit-base-patch16-224-in21k) model from HuggingFace's Transformers library. The architecture consists of:

- A ViT backbone (frozen or fine-tuned)
- A lightweight classification head (`nn.Linear`) for gesture prediction

You can optionally replace this with a **Swin Transformer**, **CNN**, or **MLP**, depending on your use case.

---

## ğŸ§¾ Requirements

List of all required packages:

```
torch
torchvision
torchaudio
transformers
opencv-python
mediapipe
numpy
```

Install via:

```bash
pip install -r requirements.txt
```

---

## ğŸ“¸ Sample Output (Webcam View)

```text
[Camera Feed]
Detected: âœŠ Fist
Detected: ğŸ–ï¸ Palm
Detected: ğŸ¤ Peace
```

Prediction text will appear at the top-left of your webcam window.

---

## ğŸ›  Tips for Customization

- Replace dummy data in `train.py` with real recorded hand landmarks and labels
- Modify `gesture_names = [...]` in `inference.py` to reflect your custom gestures
- Add more gesture classes and increase training data for better accuracy
- Try Swin Transformer by replacing the ViT model in `vit_model.py`

---

## ğŸ¤ Acknowledgements

- [MediaPipe](https://google.github.io/mediapipe/) for robust real-time hand tracking
- [HuggingFace Transformers](https://huggingface.co/transformers/) for pretrained ViT models
- [PyTorch](https://pytorch.org/) for model training and deployment

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ’¬ Questions or Contributions?

If you found this project helpful, feel free to â­ star the repo. Pull requests and issues are welcome!

